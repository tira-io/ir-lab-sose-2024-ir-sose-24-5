{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "from tira.rest_api_client import Client\n",
    "import pyterrier as pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.0 has loaded Terrier 5.8 (built by craigm on 2023-11-01 18:05) and terrier-helper 0.0.8\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    }
   ],
   "source": [
    "# Create a REST client to the TIRA platform for retrieving the pre-indexed data.\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset: the union of the IR Anthology and the ACL Anthology\n",
    "# This line creates an IRDSDataset object and registers it under the name provided as an argument.\n",
    "pt_dataset = pt.get_dataset('irds:ir-lab-sose-2024/ir-acl-anthology-20240504-training')\n",
    "\n",
    "# A (pre-built) PyTerrier index loaded from TIRA\n",
    "index = tira.pt.index('ir-lab-sose-2024/tira-ir-starter/Index (tira-ir-starter-pyterrier)', pt_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
    "\n",
    "qe = pt.rewrite.Bo1QueryExpansion(index)\n",
    "\n",
    "qe_pipeline = bm25 >> qe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unknown run format: DataFrame missing columns: ['score', 'doc_id'] (found ['query_id', 'query_0', 'query'])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mExperiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mbm25\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqe_pipeline\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBM25\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mQE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtopics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpt_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqrels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpt_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_qrels\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mndcg_cut.10\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrecip_rank\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrecall_1000\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/pipelines.py:471\u001b[0m, in \u001b[0;36mExperiment\u001b[0;34m(retr_systems, topics, qrels, eval_metrics, names, perquery, dataframe, batch_size, filter_by_qrels, filter_by_topics, baseline, test, correction, correction_alpha, highlight, round, verbose, save_dir, save_mode, **kwargs)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m     save_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.res.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m name)\n\u001b[0;32m--> 471\u001b[0m time, evalMeasuresDict \u001b[38;5;241m=\u001b[39m \u001b[43m_run_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqrels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_metrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mperquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mperquery\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackfill_qids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_topic_qids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mperquery\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpbar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m baseline \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    481\u001b[0m     evalDictsPerQ\u001b[38;5;241m.\u001b[39mappend(evalMeasuresDict)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/pipelines.py:205\u001b[0m, in \u001b[0;36m_run_and_evaluate\u001b[0;34m(system, topics, qrels, metrics, pbar, save_mode, save_file, perquery, batch_size, backfill_qids)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(res) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m topics, but no results received from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mlen\u001b[39m(topics), \u001b[38;5;28mstr\u001b[39m(system)) )\n\u001b[0;32m--> 205\u001b[0m     evalMeasuresDict \u001b[38;5;241m=\u001b[39m \u001b[43m_ir_measures_to_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mir_measures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_calc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqrels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_irmeasures_columns\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrev_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_q\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mperquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackfill_qids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m#transformer, evaluate queries in batches\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/pipelines.py:134\u001b[0m, in \u001b[0;36m_ir_measures_to_dict\u001b[0;34m(seq, metrics, rev_mapping, num_q, perquery, backfill_qids)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# measure -> value\u001b[39;00m\n\u001b[1;32m    133\u001b[0m rtr \u001b[38;5;241m=\u001b[39m {rev_mapping\u001b[38;5;241m.\u001b[39mget(m, \u001b[38;5;28mstr\u001b[39m(m)): m\u001b[38;5;241m.\u001b[39maggregator() \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m metrics}\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m seq:\n\u001b[1;32m    135\u001b[0m     metric \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mmeasure\n\u001b[1;32m    136\u001b[0m     metric \u001b[38;5;241m=\u001b[39m rev_mapping\u001b[38;5;241m.\u001b[39mget(metric, \u001b[38;5;28mstr\u001b[39m(metric))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ir_measures/providers/base.py:22\u001b[0m, in \u001b[0;36mEvaluator.iter_calc\u001b[0;34m(self, run)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03mYields per-topic metrics this run.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m expected_measure_qids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(itertools\u001b[38;5;241m.\u001b[39mproduct(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeasures, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqrel_qids))\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_calc(run):\n\u001b[1;32m     23\u001b[0m     expected_measure_qids\u001b[38;5;241m.\u001b[39mdiscard((metric\u001b[38;5;241m.\u001b[39mmeasure, metric\u001b[38;5;241m.\u001b[39mquery_id))\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m metric\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ir_measures/providers/pytrec_eval_provider.py:201\u001b[0m, in \u001b[0;36mPytrecEvalEvaluator._iter_calc\u001b[0;34m(self, run)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_iter_calc\u001b[39m(\u001b[38;5;28mself\u001b[39m, run):\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# Convert qrels to dict_of_dict (input format used by pytrec_eval)\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m     run \u001b[38;5;241m=\u001b[39m \u001b[43mir_measures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRunConverter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_dict_of_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m invoker \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvokers:\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m invoker\u001b[38;5;241m.\u001b[39miter_calc(run)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ir_measures/util.py:198\u001b[0m, in \u001b[0;36mRunConverter.as_dict_of_dict\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m     result \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m scored_doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_namedtuple_iter():\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m scored_doc\u001b[38;5;241m.\u001b[39mquery_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[1;32m    200\u001b[0m             result[scored_doc\u001b[38;5;241m.\u001b[39mquery_id] \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ir_measures/util.py:215\u001b[0m, in \u001b[0;36mRunConverter.as_namedtuple_iter\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m (ScoredDoc(sd\u001b[38;5;241m.\u001b[39mquery_id, sd\u001b[38;5;241m.\u001b[39mdoc_id, sd\u001b[38;5;241m.\u001b[39mscore) \u001b[38;5;28;01mfor\u001b[39;00m sd \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39mitertuples())\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUNKNOWN\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munknown run format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: unknown run format: DataFrame missing columns: ['score', 'doc_id'] (found ['query_id', 'query_0', 'query'])"
     ]
    }
   ],
   "source": [
    "pt.Experiment(\n",
    "    [bm25, qe_pipeline],\n",
    "    names=['BM25','QE'],\n",
    "    topics=pt_dataset.get_topics('text'),\n",
    "    qrels=pt_dataset.get_qrels(),\n",
    "    eval_metrics=[\"ndcg_cut.10\", \"recip_rank\", \"recall_1000\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The run file is normalized outside the TIRA sandbox, I will store it at \"../runs\".\n",
      "Done. run file is stored under \"../runs/run.txt\".\n"
     ]
    }
   ],
   "source": [
    "persist_and_normalize_run(run, system_name='bm25-baseline', default_output='../runs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
